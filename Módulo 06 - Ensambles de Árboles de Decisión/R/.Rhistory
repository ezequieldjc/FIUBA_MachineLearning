# Bagging and Random Forests
install.packages("randomForest")
library(randomForest)
ds = read.csv("C:/Users/charly/Desktop/UBA/UBA curso data Science/+++ curso completo POR CLASES - 1 CUATR 2019/CLASE 6 - Árboles de Decisión - Matriz Benef/Churn_Modelling.csv")
ds = read.csv("C:/Users/cardo/Desktop/Churn_Modelling.csv")
# quito campos que no uso  factorizo la variabnle objetivo
ds = ds[-c(1:3)]
ds$Exited = factor(ds$Exited, levels = c(0, 1))
str(ds)
set.seed(1)
train = sample(nrow(ds), 0.8*nrow(ds))
set.seed(1)
rf=randomForest(Exited~., data=ds , subset=train , mtry=6 , importance=TRUE , ntree=1000)
importance(rf)
help(importance)
importance(rf)[order(importance(rf)[,4], decreasing = TRUE),]
help(randomForest)
# probemos con diferebntes tamaño de árboles
cant.arb =seq(100, 1000, by=100 )
acc.vec <- double(10)
for (i in 1:length(cant.arb)) {
rf=randomForest(Exited~., data=ds , subset=train , mtry=3 , importance=TRUE,  ntree=cant.arb[i])
pred.rf.prob = predict(rf,newdata=ds[-train,], type="prob", ntree=cant.arb[i])
umbral=0.5
pred.umbral <- ifelse(pred.rf.prob[,2] > umbral, 1, 0)
acc.vec[i] = sum(diag(table(pred.umbral,ds[-train,"Exited"])))/nrow(ds[-train,])
}
plot(cant.arb,acc.vec)
lines(cant.arb,acc.vec)
#  ahora evaluando los  errores de clasificación al variar la cantidad de variables prdictoras
cant.mtry =seq(1, 10)
error.vec <- double(length(cant.mtry))
for (i in cant.mtry) {
rf=randomForest(Exited~.,data=ds,subset=train,mtry=i,importance=TRUE, ntree=300)
pred.rf.prob = predict(rf,newdata=ds[-train,], type="prob", ntree=300)
umbral=0.5
pred.umbral <- ifelse(pred.rf.prob[,2] > umbral, 1, 0)
error.vec[i] = 1-sum(diag(table(pred.umbral,ds[-train,"Exited"])))/nrow(ds[-train,])
}
matplot(cant.mtry,error.vec, pch=19, col=c("red","blue"), type = "b",
xlab = "Cant Var Predictoras", ylab="Error")
mtry_min=which.min(error.vec)
rf_optimo=randomForest(Exited~.,data=ds,subset=train,mtry=mtry_min,importance=TRUE, ntree=300)
importance(rf)[order(importance(rf)[,4], decreasing = TRUE),]
help(randomForest)
# probemos con diferebntes tamaño de árboles
cant.arb =seq(100, 1000, by=100 )
acc.vec <- double(10)
for (i in 1:length(cant.arb)) {
rf=randomForest(Exited~., data=ds , subset=train , mtry=3 , importance=TRUE,  ntree=cant.arb[i])
pred.rf.prob = predict(rf,newdata=ds[-train,], type="prob", ntree=cant.arb[i])
umbral=0.5
pred.umbral <- ifelse(pred.rf.prob[,2] > umbral, 1, 0)
acc.vec[i] = sum(diag(table(pred.umbral,ds[-train,"Exited"])))/nrow(ds[-train,])
}
plot(cant.arb,acc.vec)
lines(cant.arb,acc.vec)
#  ahora evaluando los  errores de clasificación al variar la cantidad de variables prdictoras
cant.mtry =seq(1, 10)
#  ahora evaluando los  errores de clasificación al variar la cantidad de variables prdictoras
cant.mtry =seq(1, 10)
error.vec <- double(length(cant.mtry))
for (i in cant.mtry) {
rf=randomForest(Exited~.,data=ds,subset=train,mtry=i,importance=TRUE, ntree=300)
pred.rf.prob = predict(rf,newdata=ds[-train,], type="prob", ntree=300)
umbral=0.5
pred.umbral <- ifelse(pred.rf.prob[,2] > umbral, 1, 0)
error.vec[i] = 1-sum(diag(table(pred.umbral,ds[-train,"Exited"])))/nrow(ds[-train,])
}
for (i in cant.mtry) {
print(paste0('Corriendo arbol i: ', i))
rf=randomForest(Exited~.,data=ds,subset=train,mtry=i,importance=TRUE, ntree=300)
print(paste0('Termino arbol i: ', i))
pred.rf.prob = predict(rf,newdata=ds[-train,], type="prob", ntree=300)
umbral=0.5
pred.umbral <- ifelse(pred.rf.prob[,2] > umbral, 1, 0)
error.vec[i] = 1-sum(diag(table(pred.umbral,ds[-train,"Exited"])))/nrow(ds[-train,])
}
matplot(cant.mtry,error.vec, pch=19, col=c("red","blue"), type = "b",
xlab = "Cant Var Predictoras", ylab="Error")
mtry_min=which.min(error.vec)
rf_optimo=randomForest(Exited~.,data=ds,subset=train,mtry=mtry_min,importance=TRUE, ntree=300)
# ordenada por MeanDecreaseAccuracy
importance(rf)[rev(order(importance(rf_optimo)[,3])),]
# ordenada por MeanDecreaseGini
importance(rf)[rev(order(importance(rf_optimo)[,4])),]
# Lo grafico
varImpPlot(rf_optimo)
umbral=0.5
pred.umbral <- ifelse(pred.rf.prob[,2] > umbral, 1, 0)
mc = table(pred.umbral,ds[-train,"Exited"])
mc
VP=mc[2,2];
VN=mc[1,1];
FP=mc[2,1];
FN=mc[1,2];
p_VP=VP/(VP+FN)
p_VN=VN/(VN+FP)
p_FP=FP/(VN+FP)
p_FN=FN/(VP+FN)
acc = (VP+VN)/(VP+VN+FP+FN)
p_FN
acc
