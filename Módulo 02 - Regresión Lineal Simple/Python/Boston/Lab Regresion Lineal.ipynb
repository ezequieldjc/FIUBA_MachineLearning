{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab Regresion Lineal.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mXGMF0S40Zev"},"source":["# Importar Dataset"]},{"cell_type":"code","metadata":{"id":"AQO-9f3iziJa"},"source":["from sklearn.datasets import load_boston\n","import numpy as np\n","import pandas as pd\n","\n","# Documentacion para carga de Dataset: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFUqJ3k016Qp"},"source":["boston = load_boston()\n","boston.keys()\n","\n","a = boston['data']\n","b = boston['target']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"giXHJ_RC3COe","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598455550823,"user_tz":180,"elapsed":1357,"user":{"displayName":"Ing. Carlos Arana","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2mwBvKtTbtPRg83K7tzte-qjSI81mIyMuBobcMA=s64","userId":"14092082653727502284"}},"outputId":"e8ad8ca9-374a-43b3-a181-2d8e3fcf1636"},"source":["columnas = boston['feature_names']\n","columnas = np.append(columnas, 'MEDV')\n","columnas"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n","       'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'], dtype='<U7')"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"WdvUE_2K1_eU","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598455559492,"user_tz":180,"elapsed":843,"user":{"displayName":"Ing. Carlos Arana","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2mwBvKtTbtPRg83K7tzte-qjSI81mIyMuBobcMA=s64","userId":"14092082653727502284"}},"outputId":"f2db8995-dc62-488b-caf4-73bd3de85d2d"},"source":["a.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(506, 13)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"lnNC-xxa47oW"},"source":["b.T.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eN3mkjfr3cpg"},"source":["c = np.column_stack((a,b))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjJrzxVmEu2n"},"source":["a[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DNSerjwEv5L"},"source":["b[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ZNK9n87EyA3"},"source":["c[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19pZEf3t0p09"},"source":["print('El dataset de Boston cuenta con {} filas y {} columnas'.format(c.shape[0], c.shape[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5l-yTWLe05SK"},"source":["boston_df = pd.DataFrame(c, columns = columnas)\n","boston_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q3iklE_yGpMj"},"source":["# Analizamos el Dataset"]},{"cell_type":"code","metadata":{"id":"SUc7YEtjGkyu","colab":{"base_uri":"https://localhost:8080/","height":172},"executionInfo":{"status":"error","timestamp":1618750474045,"user_tz":180,"elapsed":1224,"user":{"displayName":"Brenda Ridoldo","photoUrl":"","userId":"17286262028885629593"}},"outputId":"fb6fb5e6-aa17-48df-9994-61bdfef361c8"},"source":["boston_df.info()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cf1a5ed5a5cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mboston_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'boston_df' is not defined"]}]},{"cell_type":"code","metadata":{"id":"o-qSysGHGyAm"},"source":["boston_df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IbMkJRVLHZ3T"},"source":["# Inspeccion visual de los datos"]},{"cell_type":"markdown","metadata":{"id":"FkXJgAE2Hk86"},"source":["Dentro de las alternativas para poder visualizar datos, nos encontramos con diferentes librerias tales como Matplotlib, Seaborn, Pandas, Bokeh, etc. \n","\n","A lo largo del curso vamos a utilizar Seaborn, ya que consideramos que posee un buen balance entre versalitidad, simplicidad y funciona muy bien con Pandas. Tambien importaremos Matplotlib, ya que seaborn esta construida sobre ella, y probablemente necesitemos utilizar algunas de las funciones."]},{"cell_type":"code","metadata":{"id":"Ayzl0JT3IAhL"},"source":["import matplotlib.pyplot as plt # \n","import seaborn as sns # como importamos pandas as pd, numpy as np, searbon es comun importarla como sns."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"52qeqCT1G2rY"},"source":["sns.set_style(\"whitegrid\") # para que los graficos posean un estilo particular utilizamos sns.set_style\n","\n","fig, ax = plt.subplots(figsize = (20,10))\n","\n","\n","sns.scatterplot(x = 'LSTAT',\n","                y = 'MEDV',\n","                data = boston_df,\n","                ax = ax\n","                )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C2_00nvPKP7J"},"source":["# Regresion lineal\n"]},{"cell_type":"markdown","metadata":{"id":"sCdymSPvKYav"},"source":["Como vimos en el grafico anterior, hace sentido que haya una relacion entre LSTAT y MEDV, con lo cual podemos proceder a estimar una regresion lineal.\n","\n","Para esto, necesitaremos importar otra libreria de sklearn que nos permita hacer los calculos correspondientes.\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n","\n","A su vez, nos vamos a encontrar con un problema si intentamos pasarle a los modelos de sklearn una serie de pandas, ya que solo recibe Numpy Arrays, pero eso es muy facil de convertir con pandas, simplemente necesitamos utilizar el metodo **.values** sobre la columna que deseemos.\n","\n","Es importante tener en consideracion los siguientes conceptos, vistos en la teoria:\n","\n","- R2\n","- Residuos\n"]},{"cell_type":"code","metadata":{"id":"5xbjM7gnKlY-"},"source":["from sklearn.linear_model import LinearRegression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tMXlmnwFK9OZ"},"source":["reg = LinearRegression() # Inicializamos una instancia de regresion lineal en el parametro reg, esto nos va a permitir entrenar al modelo y luego utilizarlo para predecir."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3lyULZTL7NZ"},"source":["X = boston_df['LSTAT'].values.reshape(-1, 1)\n","y = boston_df['MEDV'].values\n","\n","reg.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4KD0bZ2uPiHd"},"source":["Ya tenemos inicializada nuestra instacia de regresion lineal, y la hemos entrenado con los datos de LSTAT y MEDV, ahora deberiamos poder obtener los parametros para poder construir nuestra recta, recordemos que la funcion de la recta es:\n","\n","      y = m * x + b\n","\n","\n","Donde b es la ordenada al origen y m la pendiente de la recta."]},{"cell_type":"code","metadata":{"id":"MJNXNZQkOdvR"},"source":["m = reg.coef_[0]\n","\n","print('El valor de la pendiente de la recta es: {}'.format(m))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1HyRrFlUQ2nJ"},"source":["b = reg.intercept_\n","\n","print('El valor de la ordenada al origen de la recta es: {}'.format(b))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVRPvxX5G0Fz"},"source":["# ahora formulemos la recta\n","\n","y_pred = m * X + b\n","y_pred[:20].reshape(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyXr0bq7RkdV"},"source":["# tambien podemos utilizar una funcionalidad de la libreria llamada predict, esto nos simplifica el hecho de realizar la formula, verifiquemos si llegamos al mismo resultado\n","\n","y_pred2 = reg.predict(X)\n","y_pred2[:20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqeE8MDhTaF2"},"source":["# para obtener los residuos, simplemente restamos los valores de y reales, con los y predichos\n","\n","residuos = y.reshape(-1) - y_pred.reshape(-1)\n","residuos.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86jodQhoVcxN"},"source":["# para analizarlo estadisticamente lo convertiremos en una serie de pandas y luego utilizaremos el metodo \"describe\"\n","# tambien se pueden utilizar las funciones de Numpy para resumir estadisticamente al df, queda a criterio y facilidad de cada persona\n","\n","pd.Series(residuos.reshape(-1)).describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrvy0SMKSAd0"},"source":["# SCT es las suma total de los cuadrados de la distancia entre el valor de las y de la muestra y su media muestral y_raya\n","# SCR es la suma total dela diferencia elevada al cuadrado de la distancia entre las y de la muestra y las y ajustadas por el modelo\n","y_pred = (m * X + b).reshape(-1)\n","SCT = sum((y - np.mean(y)) ** 2)\n","SCR = sum((y - y_pred) ** 2)\n","\n","r2 = 1 - (SCR/SCT)\n","print('El valor es de R cuadrado es {}'.format(r2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"al2p7WhTWQQB"},"source":["# ahora nos queda obtener el valor de R2\n","\n","reg.score(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lS4wESxeah6E"},"source":["Ahora vamos a imprimir tanto los datos como la recta del modelo generado"]},{"cell_type":"code","metadata":{"id":"zJIetdSIX1nd"},"source":["x_recta = np.linspace(0, 40, 100)\n","\n","y_recta = m * x_recta + b\n","\n","fig, ax = plt.subplots(2, figsize = (20,15))\n","\n","# en el primer grafico insertamos un scatterplot con los residuos\n","ax[0].scatter(X, residuos)\n","ax[0].set_title('Residuos')\n","\n","# en el segundo insertamos tanto la recta resultante del modelo de regresion lineal como los datos iniciales que utilizamos para \n","ax[1].plot(x_recta, y_recta, c = 'r')\n","sns.scatterplot(x = 'LSTAT',\n","                y = 'MEDV',\n","                data = boston_df,\n","                ax = ax[1]\n","                )\n","ax[1].set_title('Regresion lineal')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVNGLWyvk2ri"},"source":["# dato interesante para entender la funcion de plt.subplots(), la variable \"ax\", es una tupla del tamaño de el numero indicado. Dada esta propiedad podemos descomponerla la tupla en la misma formula \n","\n","fig, (ax0, ax1) = plt.subplots(2, figsize = (20,15))\n","\n","# en el primer grafico insertamos un scatterplot con los residuos\n","ax0.scatter(X, residuos)\n","ax0.set_title('Residuos')\n","\n","# en el segundo insertamos tanto la recta resultante del modelo de regresion lineal como los datos iniciales que utilizamos para \n","ax1.plot(x_recta, y_recta, c = 'r')\n","sns.scatterplot(x = 'LSTAT',\n","                y = 'MEDV',\n","                data = boston_df,\n","                ax = ax1\n","                )\n","ax1.set_title('Regresion lineal')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iS45Ro6tuxVA"},"source":["# Predecir un valor"]},{"cell_type":"markdown","metadata":{"id":"7izmiRUIANKZ"},"source":["Si queremos predecir un valor podemos ir nuevamente por dos caminos:\n","\n","- Reemplazar los valores en la formula obtenida\n","- Utilizar el método **.predict()** sobre el modelo generado\n","\n","(recoredemos que es necesario pasar los datos como numpy arrays para que la libreria lo tome correctamente)\n","\n","Probemos predecir un valor, y luego, una serie de valores. Vamos a definir las variables en la siguiente linea de codigo.\n","\n","Una vez terminado el ejercicio, verifiquemos si graficamente nos hace sentido el resultado."]},{"cell_type":"code","metadata":{"id":"5ypjW0MvBcjb"},"source":["lstat_unica = np.array(5).reshape(-1, 1) # el reshape lo hacemos para que el formato del arreglo tenga formato definido\n","\n","lstat_mult = np.array([5, 10, 20, 30]).reshape(-1, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sU93iso8AMjY"},"source":["# utilizamos el metodo para obtener el dato a traves de reemplazar los arreglos previos en la formula\n","\n","lstat_pred_unica = m * lstat_unica + b\n","\n","lstat_pred_mult = m * lstat_mult + b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SJKv_8SDDmI"},"source":["# verificamos la prediccion de un valor simple\n","\n","lstat_pred_unica"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8EW67AsDFWE"},"source":["# verificamos la prediccion de un arreglo\n","\n","lstat_pred_mult.reshape(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSwZ43truoS0"},"source":["# ahora vamos a utilizar el metodo .predict() sobre nuestro modelo ya entrenado para un caso simple\n","\n","reg.predict(lstat_unica)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDgJo9mWCb2o"},"source":["# ahora probamos lo mismo pero con el arreglo multiple.\n","\n","reg.predict(lstat_mult)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aBoqDDIvEIdt"},"source":["# Conclusiones"]},{"cell_type":"markdown","metadata":{"id":"4cEaDz83EKDV"},"source":["Como vimos hoy hay muchas formas diferentes de como llegar al mismo resultado, dependiendo el tipo de problema siempre va a haber una alternativa mas simple para su resolucion.\n","\n","Invitamos a revisar las librerias de sklearn y numpy, ya que van a ser muy necesarias para las siguientes practicas."]}]}